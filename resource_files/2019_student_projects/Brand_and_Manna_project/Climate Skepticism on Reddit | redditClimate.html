<!DOCTYPE html>
<html lang="en-US"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <link rel="stylesheet" type="text/css" media="screen" href="Climate%20Skepticism%20on%20Reddit%20|%20redditClimate_files/style.css">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Climate Skepticism on Reddit | redditClimate</title>
<meta name="generator" content="Jekyll v3.8.5">
<meta property="og:title" content="Climate Skepticism on Reddit">
<meta property="og:locale" content="en_US">
<link rel="canonical" href="https://redditclimate.github.io/redditClimate/">
<meta property="og:url" content="https://redditclimate.github.io/redditClimate/">
<meta property="og:site_name" content="redditClimate">
<script type="application/ld+json">
{"@type":"WebSite","url":"https://redditclimate.github.io/redditClimate/","name":"redditClimate","headline":"Climate Skepticism on Reddit","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/RedditClimate/redditClimate">View on GitHub</a>

          <h1 id="project_title">redditClimate</h1>
          <h2 id="project_tagline"></h2>

          
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1 id="climate-skepticism-on-reddit">Climate Skepticism on Reddit</h1>
<h2 id="introduction">Introduction</h2>

<p>In this project, we set out to explore climate skepticism as it manifests on <a href="https://www.reddit.com/">Reddit</a>. Our goals for this project are two-fold:</p>

<ol>
  <li>Gain a deeper understanding of climate skepticism in a data-driven way</li>
  <li>Empower others to use Reddit data for research</li>
</ol>

<p>We present a number of experiments which were carried out using the 
Pushshift Reddit API, provide a detailed walkthrough of the code so that
 others can recreate and extend our results, and endevour to visualize 
and analyze the data.</p>

<p>To our knowledge, this is the first data-driven analysis of climate skepticism on Reddit. All of the code for this project is <a href="https://github.com/redditclimate/redditClimate">available on GitHub</a>.</p>

<h2 id="background">Background</h2>

<p>The role of humans in climate change was demonstrated over 100 years ago by Svante Arrhenius <a href="https://www.rsc.org/images/Arrhenius1896_tcm18-173546.pdf">[ref]</a>,
 and since then a formidable body of scientific evidence has accumulated
 leading over 97% of scientists to conclude that human activity has a 
significant impact on the climate <a href="https://skepticalscience.com/global-warming-scientific-consensus-intermediate.htm">[ref]</a>.
 An ever-improving collection of climate models even allow us to 
estimate the exact degree to which we will cause the planet to warm, and
 the details of how this will impact specific ecosystems and industries.
 Increasingly, these predictions are being validated by an increase in 
extreme weather events, collapsing ecosystems, and warming temperatures 
around the globe.</p>

<p>Despite the overwhelming evidence of anthropogenic global warming 
(AGW), in general we have been incredibly slow to take significant 
action to mitigate our impact on the climate. There are <strong>many</strong>
 factors contributing to this hesitation — one of which is widespread 
skepticism or denial of the reality of AGW. For certain individuals and 
companies (most notably fossil-fuel industries), avoiding climate action
 is a profitable strategy in the short term. As a result, they have 
mounted numerous campains to spread misinformation and discredit climate
 science <a href="https://www.merchantsofdoubt.org/">[ref]</a>.</p>

<p>Moving the needle on climate action will likely require us to 
convince climate skeptics of the reality and severity of this issue. 
Before we can do so effectively it is critical to understand why and how
 people deny AGW, and so we turn to the treasure trove of content 
generated by climate skeptics on the internet.</p>

<p>We considered doing an anlysis of Twitter or Facebook, as we believe 
those platforms to be largely representative of the general population. 
However, significant research has been done on those platforms in the 
past (discussed in the Related Work section). In the interest of making a
 novel contribution, we decided to study climate skepticism on Reddit.</p>

<h3 id="sample-bias">Sample Bias</h3>

<p>A taste of Reddit demographics <a href="https://www.techj unkie.com/demographics-reddit/">[ref]</a>:</p>

<ul>
  <li>67-69% male</li>
  <li>54% US-based users</li>
  <li>Similar racial distribution to US pop.</li>
  <li>58% age 18-29,	33% age 30-49</li>
</ul>

<p>So Reddit appears to be skewed towards males and a younger demographic. We also found from various sources on Reddit, <a href="https://grist.org/climate-energy/reddits-science-forum-banned-climate-deniers-why-dont-all-newspapers-do-the-same/">in the news</a>, and in our data, that Reddit is skewed liberal. This selection bias is worth keeping in mind throughout the analysis.</p>

<h2 id="pushshift-api">Pushshift API</h2>

<p>Reddit is special among the large social-media platforms in that it 
provides a free, extensive API for interacting with content on the 
platform. The API exposes nearly all the functionality that a regular 
user would have when browsing reddit.</p>

<p>The pushshift API has two active endpoints, which can be found at:</p>

<ol>
  <li>Comment Endpoint: <a href="https://api.pushshift.io/reddit/search/comment">https://api.pushshift.io/reddit/search/comment</a></li>
  <li>Submission Endpoint: <a href="https://api.pushshift.io/reddit/search/submission">https://api.pushshift.io/reddit/search/submission</a></li>
</ol>

<p>Try following these links and inspect the results in your browser. 
For the comments endpoint, you should see something like this:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"data"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="p">{</span><span class="w">
            </span><span class="nl">"all_awardings"</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span><span class="w">
            </span><span class="nl">"approved_at_utc"</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span><span class="w">
            </span><span class="nl">"associated_award"</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span><span class="w">
            </span><span class="nl">"author"</span><span class="p">:</span><span class="w"> </span><span class="s2">"12UglyTacos"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"author_flair_background_color"</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span><span class="w">
            </span><span class="nl">"author_flair_css_class"</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span><span class="w">
            </span><span class="nl">"author_flair_richtext"</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span><span class="w">
            </span><span class="nl">"author_flair_template_id"</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span><span class="w">
            </span><span class="nl">"author_flair_text"</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span><span class="w">
            </span><span class="nl">"author_flair_text_color"</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span><span class="w">
            </span><span class="nl">"author_flair_type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"text"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"author_fullname"</span><span class="p">:</span><span class="w"> </span><span class="s2">"t2_g0vwsy8"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"author_patreon_flair"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w">
            </span><span class="nl">"author_premium"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w">
            </span><span class="nl">"awarders"</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span><span class="w">
            </span><span class="nl">"banned_at_utc"</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span><span class="w">
            </span><span class="nl">"body"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Good luck girl! Let us know how you do! </span><span class="se">\n\n</span><span class="s2">&amp;lt;3"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"can_mod_post"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w">
            </span><span class="nl">"collapsed"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w">
            </span><span class="nl">"collapsed_because_crowd_control"</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span><span class="w">
            </span><span class="nl">"collapsed_reason"</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span><span class="w">
            </span><span class="nl">"created_utc"</span><span class="p">:</span><span class="w"> </span><span class="mi">1575947859</span><span class="p">,</span><span class="w">
            </span><span class="nl">"distinguished"</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span><span class="w">
            </span><span class="nl">"edited"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w">
            </span><span class="nl">"gildings"</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span><span class="w">
            </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"facv822"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"is_submitter"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span><span class="w">
            </span><span class="nl">"link_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"t3_e72kna"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"locked"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w">
            </span><span class="nl">"no_follow"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span><span class="w">
            </span><span class="nl">"parent_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"t1_f9xve3h"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"permalink"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/r/entwives/comments/e72kna/decided_my_2020_resolution_was_to_learn_to_french/facv822/"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"retrieved_on"</span><span class="p">:</span><span class="w"> </span><span class="mi">1575947860</span><span class="p">,</span><span class="w">
            </span><span class="nl">"score"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
            </span><span class="nl">"send_replies"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span><span class="w">
            </span><span class="nl">"steward_reports"</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span><span class="w">
            </span><span class="nl">"stickied"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w">
            </span><span class="nl">"subreddit"</span><span class="p">:</span><span class="w"> </span><span class="s2">"entwives"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"subreddit_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"t5_2s7a6"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"total_awards_received"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="err">...</span><span class="w">
</span></code></pre></div></div>

<p>As we can see from the JSON result, each comment has a lot of 
associated information that might be useful to us. Here are some of the 
fields that stand out as particularly useful:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">"body"</code>: the actual text of the comment</li>
  <li><code class="language-plaintext highlighter-rouge">"author"</code>: the username of the user who submitted the comment</li>
  <li><code class="language-plaintext highlighter-rouge">"subreddit"</code>: the subreddit that the comment is in</li>
  <li><code class="language-plaintext highlighter-rouge">"created_utc"</code>: a unix timestamp indicating when the comment was created</li>
</ul>

<p>The result returned by the submission endpoint has a similar 
structure, but with different fields for each submission. Here’s a 
descripton of some of the relevant fields for each submission:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">"title"</code>: the title of the submission</li>
  <li><code class="language-plaintext highlighter-rouge">"selftext"</code>: the body of text inside the submission (optional: not all submissions have selftext)</li>
  <li><code class="language-plaintext highlighter-rouge">"domain"</code>: the domain of a website that the submission links to (optional: not all submissions have domains)</li>
  <li>“<code class="language-plaintext highlighter-rouge">subreddit_subscribers</code>”: the number of members in the subreddit of this submission</li>
  <li><code class="language-plaintext highlighter-rouge">"author"</code>: the username of the user who submitted the submission</li>
  <li><code class="language-plaintext highlighter-rouge">"subreddit"</code>: the subreddit that the comment is in</li>
  <li><code class="language-plaintext highlighter-rouge">"created_utc"</code>: a unix timestamp indicating when the submission was created</li>
</ul>

<p>We can farther filter the results from each endpoint using API parameters. The full list of parameters can be found <a href="https://pushshift.io/api-parameters/">here</a>.
 Suppose we want to look at the titles and authors of posts in 
r/conservative which mention “climate change.” Our parameters would be</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">filter=author,title</code> to return only the authors and titles</li>
  <li><code class="language-plaintext highlighter-rouge">q=climage%20change</code> to query for content containing the phrase “climate change” (<code class="language-plaintext highlighter-rouge">%20</code> is ASCII for a space)</li>
  <li><code class="language-plaintext highlighter-rouge">subreddit=conservative</code> to search within the r/conservative subreddit</li>
</ul>

<p>We compose all of these parameters by</p>

<p align="center"> <code>api url</code> + <code>?</code> + <code>param1</code> + <code>&amp;</code> + <code>param2</code> + <code>&amp;</code> + <code>param3</code> ...</p>

<p>So our finished URL would look like:</p>

<p align="center"> <a href="http://api.pushshift.io/reddit/search/submission?q=climate%20change&amp;filter=author,title&amp;subreddit=conservative">http://api.pushshift.io/reddit/search/submission?q=climate%20change&amp;filter=author,title&amp;subreddit=conservative</a> </p>

<p>And if we visit that URL in browser we get</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
    "data": [
        {
            "author": "OneBernie2020Please",
            "title": "Nuclear power key to solving climate change"
        },
        {
            "author": "optionhome",
            "title": "Teenage climate activist Greta Thunberg is a stunning inspiration to us all. She is in her 18th month on strike from school. \u201cYeah, uh, climate change or whatever it\u2019s called is super important to me,\u201d says James Davidson, 17, \u201cso that\u2019s definitely why I stopped going to school two years ago."
        },
        {
            "author": "Foubar",
            "title": "Elizabeth Warren Unveils Plan To End Climate Change By Performing Authentic Rain Dance"
        },
        {
            "author": "YanksSensBills",
            "title": "What do you think the GOP can do to address climate change?"
        },
        {
            "author": "Foubar",
            "title": "Climate Alarmists Want to Rebrand \"Climate Change\" to Something More Panic Inducing"
        },
        ...
</code></pre></div></div>

<p>If we go to <a href="https://api.pushshift.io/meta">https://api.pushshift.io/meta</a>,
 we’ll see that the Pushshift API has a rate limit of 120 requests per 
minute - that’s one every 0.5 seconds. Therefore, we will want to slow 
our requests down by waiting 0.5 seconds between requests.</p>

<p>The pushshift API caps the number of results returned for a single 
request to 1000. Each result contains data about either a comment or a 
submission depending on the endpoint queried.</p>

<h2 id="experiments">Experiments</h2>

<h3 id="making-simple-queries">Making Simple Queries</h3>

<p>Let’s start by using python to programatically make a request.</p>

<p>First, we’ll want to import python’s <code class="language-plaintext highlighter-rouge">requests</code> library for making API requests, as well as the <code class="language-plaintext highlighter-rouge">time</code> library so that we can make sure not to exceed the Pushshift API’s rate limit.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">time</span>
</code></pre></div></div>

<p>Now, let’s write a function called <code class="language-plaintext highlighter-rouge">query</code> which takes in as input the name of the endpoint, which should be <code class="language-plaintext highlighter-rouge">"comment</code> or <code class="language-plaintext highlighter-rouge">"submission"</code>,
 as well as a dictionary of query parameters. The function should query 
the endpoint with the given parameters and return the result as a list 
of dictionaries.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">query</span><span class="p">(</span><span class="n">endpoint</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    
    <span class="n">params_string</span> <span class="o">=</span>\
        <span class="s">"&amp;"</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">f</span><span class="s">"{param}={val}"</span> <span class="k">for</span> <span class="n">param</span><span class="p">,</span><span class="n">val</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
    <span class="n">url</span> <span class="o">=</span> <span class="n">f</span><span class="s">"https://api.pushshift.io/reddit/search/{endpoint}/?{params_string}"</span>

    <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span> <span class="o">=</span> <span class="n">url</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">[</span><span class="s">"data"</span><span class="p">]</span>
</code></pre></div></div>
<p>This works great if we want up to 1000 results, but if we try to ask 
for more than 1000 results, this will not work because of pushshift’s 
size limit. This means that we’ll have to make multiple requests until 
we have the desired number of results. For example, if we want to get 
2500 results, then we can make 3 API calls, querying for 1000 results in
 the first 2, and 500 results in the third.</p>

<p>We’ll also want to call time.sleep with an argument of 0.5 to tell python to wait 0.5 seconds after each request.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">query_n</span><span class="p">(</span><span class="n">endpoint</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="n">params</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s">"sort_type"</span><span class="p">:</span> <span class="s">"created_utc"</span><span class="p">,</span> <span class="s">"sort"</span><span class="p">:</span><span class="s">"desc"</span><span class="p">,</span> <span class="s">"size"</span><span class="p">:</span><span class="n">n</span><span class="p">})</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">:</span>
        <span class="n">query_res</span> <span class="o">=</span> <span class="n">query</span><span class="p">(</span><span class="n">endpoint</span><span class="p">,</span> <span class="p">{</span><span class="o">**</span><span class="n">params</span><span class="p">,</span> <span class="s">"before"</span><span class="p">:</span> <span class="n">results</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s">"created_utc"</span><span class="p">]</span> <span class="k">if</span> <span class="n">results</span> <span class="k">else</span> <span class="nb">int</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())</span> <span class="p">})</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">query_res</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">results</span><span class="err">!</span><span class="p">[](</span><span class="err">!</span><span class="p">[]())</span>
        <span class="n">results</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">query_res</span><span class="p">)</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div>

<p>Great! Now that we know how to make requests, let’s dive into the data.</p>

<h3 id="topic-modelling">Topic Modelling</h3>

<p>In natural language processing, topic modeling is the task of 
identifying topics within a collection of documents. Here, we use the 
word document to refer to any chunk of text which could be made up of 
arbitrarily many sentences. Given a collection of documents and a 
desired number of topics <code class="language-plaintext highlighter-rouge">num_topics</code>, a topic modeling algorithm can produce <code class="language-plaintext highlighter-rouge">num_topics</code>
 topics, each of which is characterized by a distribution over the 
vocabulary of words. For example, a topic could be characterized by the 
following distribution of words: 50% <code class="language-plaintext highlighter-rouge">"composting"</code>, 40% <code class="language-plaintext highlighter-rouge">"sustainability"</code>, 10% <code class="language-plaintext highlighter-rouge">"recycling"</code>”.
 These words and their respective weightings indicate that one of the 
topics discussed by a significant proportion of documents involves 
composting and sustainability, with some mentions of recycling.</p>

<p>We’ll be using Latent Dirichlet Allocation (LDA), which is one of these topic modelling algorithms.</p>

<p>In order to perform topic modeling, we’ll need a few python libraries:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># For Preprocessing
</span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">RegexpTokenizer</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">nltk.stem.porter</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>
<span class="c1"># For Latent Dirichlet Allocation
</span><span class="kn">import</span> <span class="nn">gensim</span>
</code></pre></div></div>

<p>Next, let’s define a function that will preprocess the documents and perform topic modelling:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">lda</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">num_topics</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_words</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">num_passes</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
	<span class="c1"># Setup
</span>	<span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s">'english'</span><span class="p">))</span>
	<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">RegexpTokenizer</span><span class="p">(</span><span class="s">r'\w+'</span><span class="p">)</span>
	<span class="n">porter_stemmer</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>

	<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
		<span class="c1"># 1. Lowercase and tokenize each document
</span>		<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
		<span class="c1"># 2. Remove any stop words
</span>		<span class="n">stopped_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">tok</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">tok</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>
		<span class="c1"># 3. Apply porter stemming
</span>		<span class="n">docs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">porter_stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">stopped_tok</span><span class="p">)</span> <span class="k">for</span> <span class="n">stopped_tok</span> <span class="ow">in</span> <span class="n">stopped_tokens</span><span class="p">]</span>
    
	<span class="c1"># Create a dictionary and a corpus for LDA
</span>	<span class="n">dictionary</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">corpora</span><span class="o">.</span><span class="n">Dictionary</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
	<span class="n">corpus</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">dictionary</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">,</span> <span class="n">docs</span><span class="p">))</span>
	<span class="c1"># Train the LDA model
</span>	<span class="n">lda_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">ldamodel</span><span class="o">.</span><span class="n">LdaModel</span><span class="p">(</span>
					<span class="n">corpus</span><span class="p">,</span>
					<span class="n">num_topics</span> <span class="o">=</span> <span class="n">num_topics</span><span class="p">,</span>
					<span class="n">id2word</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">,</span>
					<span class="n">passes</span> <span class="o">=</span> <span class="n">num_passes</span><span class="p">)</span>
	<span class="c1"># Return the generated topics
</span>	<span class="k">return</span> <span class="n">lda_model</span><span class="o">.</span><span class="n">print_topics</span><span class="p">(</span><span class="n">num_topics</span><span class="o">=</span><span class="n">num_topics</span><span class="p">,</span> <span class="n">num_words</span> <span class="o">=</span> <span class="n">num_words</span><span class="p">)</span>
</code></pre></div></div>

<p>At this point, we’re ready to apply LDA to a subreddit. In the case 
of reddit, we want to know what different submissions and comments are 
discussing, so each document for LDA can be submission or a comment from
 the subreddit we are investigating.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">topic_modeling_within_subreddit</span><span class="p">(</span><span class="n">subreddit</span><span class="p">,</span> <span class="n">num_topics</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">"Creating docs for subreddit: {subreddit}"</span> <span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">query_n</span><span class="p">(</span><span class="s">"submission"</span><span class="p">,</span> <span class="p">{</span><span class="s">"subreddit"</span><span class="p">:</span> <span class="n">subreddit</span><span class="p">},</span>  <span class="n">n</span> <span class="o">=</span> <span class="mi">25000</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">query_n</span><span class="p">(</span><span class="s">"comment"</span><span class="p">,</span> <span class="p">{</span><span class="s">"subreddit"</span><span class="p">:</span> <span class="n">subreddit</span><span class="p">},</span>  <span class="n">n</span> <span class="o">=</span> <span class="mi">25000</span><span class="p">))</span>
    <span class="n">text_fields</span> <span class="o">=</span> <span class="p">[</span><span class="s">"title"</span><span class="p">,</span> <span class="s">"selftext"</span><span class="p">,</span> <span class="s">"body"</span><span class="p">]</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">result</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">field</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span> <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">text_fields</span><span class="p">])</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">lda</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">num_topics</span> <span class="o">=</span> <span class="n">num_topics</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s try it out! We can start by asking for the 10 topics that best 
characterize r/climateskeptics, and compare it with the 10 topics that 
best characterize r/sustainability.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">subreddit</span> <span class="ow">in</span> <span class="p">(</span><span class="s">"climateskeptics"</span><span class="p">,</span> <span class="s">"sustainability"</span><span class="p">):</span>
	<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">"Topics for subreddit: {subreddit}"</span><span class="p">)</span>
	<span class="k">print</span><span class="p">(</span><span class="n">topic_modeling_within_subreddit</span><span class="p">(</span><span class="n">subreddit</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="subreddit-similarity">Subreddit Similarity</h3>

<p>Each subreddit represents a community, so a natural line of inquiry 
is to examine how various subreddits relate to eachother. Perhaps the 
simplest approach to this task is to examine the overlap in members 
between two subreddits. On Reddit, an invididual’s subscriptions to 
various subreddits are kept anonmymous. We were able to find <a href="https://www.redditinvestigator.com/">third party tools</a>
 which can scrape subscription information for individual users, but 
each query takes on the order of minutes. A much quicker approach to 
proxy subscription information is to see which users have made 
submissions or comments in a subreddit.</p>

<p>For each subreddit that we would like to investigate we</p>

<ol>
  <li>pull the <em>n</em> most recent posts (submissions or comments)</li>
  <li>Find the unique authors which created those posts (using a python <code class="language-plaintext highlighter-rouge">set</code>)</li>
</ol>

<p>We now have a set of of contributors for each subreddit. We can now consider simple set-comparison metrics, like the <a href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard Index</a> (aka Intersection over Union)</p>

<p align="center">
  <img src="Climate%20Skepticism%20on%20Reddit%20|%20redditClimate_files/eaef5aa86949f49e7dc6b9c8c3dd8b233332c9e7.svg" alt="Intersection over union">
</p>

<p>which is a measure of similarity between sets <em>A</em> and <em>B</em>. We define the Jaccard index in <a href="https://github.com/IzzyBrand/redditClimate/blob/master/clustering_subreddits.py">clustering_subreddits.py</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">intersection_over_union</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">a</span> <span class="o">&amp;</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">a</span> <span class="o">|</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div>

<p>The function <code class="language-plaintext highlighter-rouge">get_authors(subreddit)</code> defined in <a href="https://github.com/IzzyBrand/redditClimate/blob/master/scrape_members.py">scrape_members.py</a>
 pulls a unique list of authors for the specified subreddit by scraping 
the history of content for that subreddit. So let’s test out our 
subreddit similarity metric!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">get_authors</span><span class="p">(</span><span class="s">'climateskeptics'</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">get_authors</span><span class="p">(</span><span class="s">'environmental_science'</span><span class="p">))</span>
<span class="n">c</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">get_authors</span><span class="p">(</span><span class="s">'conservative'</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">'r/climateskeptics with r/environmental_science'</span><span class="p">,</span>\
	<span class="n">intersection_over_union</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'r/climateskeptics with r/conservative'</span><span class="p">,</span>\
	<span class="n">intersection_over_union</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">c</span><span class="p">))</span>
</code></pre></div></div>

<p>This prints</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>r/climateskeptics with r/environmental_science 0.006036217303822937
r/climateskeptics with r/conservative 0.015847216578626575
</code></pre></div></div>

<p>Indicating that the <em>r/climateskeptics</em> community has more in common with <em>r/conservative</em> than it does with <em>r/environmental_science</em>.</p>

<h3 id="clustering-subreddits">Clustering Subreddits</h3>

<p>Ideally we wouldn’t have to hand-pick pairs of subreddits to compare,
 because in doing so we are potentially restricting ourselves to only 
find trends which we already believe to be present in the data. In order
 to cast a wider net, we’d like to pursue an analysis which compares 
many subreddits at once, and discovers trends on its own. In this 
experiment we consider clustering subreddits based on their membership 
sets (from the previous section) and visualizing those clusters with 
dimensionality reduction.</p>

<p>In particular, we will use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">sklearn’s implementation of PCA</a> for dimensionality reduction and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">KMeans</a> for clustering, though any number of alternative methods may apply here.</p>

<p>Both of these methods expect data points with vector-representations,
 but we currently have sets of members for each subreddit. In order to 
create a vector for each subreddit, we first create a set consisting of 
all the members which have posted across all the subreddits. Let’s say 
this set is size, <em>N</em>. Then, for each subreddit, we create a vector of length <em>N</em>, where the ith element in the vector is a 1 if the ith member is in that subreddit, and a 0 otherwise.</p>

<p>For example, if users alice and candice have posted in 
r/climateskeptics but bob, and david have not, the vector for the 
r/climateskeptics subreddit would look like the first row of this table.</p>

<table>
  <thead>
    <tr>
      <th>&nbsp;</th>
      <th>alice</th>
      <th>bob</th>
      <th>candice</th>
      <th>david</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>r/climateskeptics</strong></td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td><strong>r/conservative</strong></td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <td><strong>r/environmental_science</strong></td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>If we have vectors for multiple subreddits, we end up building a 
matrix, where each row is a vector for a subreddit, and each column is a
 vector indicating where each user has been active. The code to generate
 this matrix lives in <a href="https://github.com/IzzyBrand/redditClimate/blob/master/clustering_subreddits.py">clustering_subreddits.py</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_matrix</span><span class="p">(</span><span class="n">subreddits</span><span class="p">):</span>
    <span class="n">membership_list</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1"># dict from subreddit -&gt; membership set
</span>    <span class="n">authors</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span> <span class="c1">#      set containing all the authors accross subreddits
</span>
    <span class="c1"># for each subreddit, get the membership set
</span>    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">subreddits</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Pulling authors for {}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">get_authors</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="s">'submission'</span><span class="p">,</span> <span class="n">max_num_authors</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>
        <span class="n">authors</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">membership_list</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span>

    <span class="n">authors</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">authors</span><span class="p">)</span>
    <span class="n">n_subreddits</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">subreddits</span><span class="p">)</span>
    <span class="n">n_authors</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">authors</span><span class="p">)</span>

    <span class="c1"># instantiate the matrix of zeros, and mark 1s as needed
</span>    <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">n_subreddits</span><span class="p">,</span> <span class="n">n_authors</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">subreddits</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">authors</span><span class="p">):</span>
            <span class="n">M</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">membership_list</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>

    <span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">subreddits</span><span class="p">,</span> <span class="n">authors</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span>
</code></pre></div></div>

<p>This function takes a while to run, depending on how many authors we request from <code class="language-plaintext highlighter-rouge">get_authors</code> (defined in <a href="https://github.com/IzzyBrand/redditClimate/blob/master/scrape_members.py">scrape_members.py</a>),
 and how many subreddits we pass in the argument list. Once we’ve built 
the matrix, we can use unsupervised learning to cluster subreddits based
 on their similarity. In this case, we’ll explore sklearn’s <code class="language-plaintext highlighter-rouge">KMeans</code>.
 It’s worth noting that KMeans exhibits the implicit prior that each of 
our clusters should be roughly the same size, which may or may not be a 
valid assumption depending on the data…</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">cluster_matrix</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">subreddits</span><span class="p">,</span> <span class="n">authors</span><span class="p">,</span> <span class="n">M</span> <span class="o">=</span> <span class="n">data</span>
    <span class="k">return</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
</code></pre></div></div>

<p>This function will return a vector of labels, one for each subreddit.
 Each label will be an integer specifying which cluster the subreddit 
has been assigned to, where the argument <em>n</em> specifies the number
 of clusters to build. We could print out and read these clusters of 
subreddits, but let’s take things a step farther and try to visualize 
our data. Unfortunately, we can’t directly plot high-dimensional data 
(the dimensionality of our data is the number of unique users). Instead,
 we’ll use PCA to project our data into a lower dimension. We’re using <code class="language-plaintext highlighter-rouge">matplotlib</code> to scatterplot the results, and we color each point according to the <code class="language-plaintext highlighter-rouge">labels</code> obtained with KMeans.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pca_matrix</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">subreddits</span><span class="p">,</span> <span class="n">authors</span><span class="p">,</span> <span class="n">M</span> <span class="o">=</span> <span class="n">data</span>

    <span class="c1"># project the matrix into two dimensions
</span>    <span class="n">pca</span> <span class="o">=</span> <span class="n">KernelPCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
    <span class="n">projected</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>

    <span class="c1"># and plot the results!
</span>    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">projected</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">subreddits</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">projected</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Putting it all together</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">generate_matrix</span><span class="p">(</span><span class="n">subreddits</span><span class="p">)</span> 
<span class="n">data</span> <span class="o">=</span> <span class="n">remove_too_small</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">900</span><span class="p">)</span> <span class="c1"># some subreddits don't have enough members
</span><span class="n">y</span> <span class="o">=</span> <span class="n">cluster_matrix</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="c1"># cluster with 4 clusters
</span><span class="n">pca_matrix</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>
<p>where <code class="language-plaintext highlighter-rouge">subreddits</code> is a list of subreddits and <code class="language-plaintext highlighter-rouge">remove_too_small</code> removes subreddit vectors with fewer than a certain number of members. This generates the plot:</p>

<p align="center">
 	<img src="Climate%20Skepticism%20on%20Reddit%20|%20redditClimate_files/4_cluster_pca.png" width="90%">&lt;/img&gt;
</p>

<p>What does this mean? The 4 clusters don’t have a set meaning, as they
 were discovered in an unsupervised manner by KMeans, but we can attempt
 to interpret them by looking at the labeled subreddits</p>

<p align="center">
	<img src="Climate%20Skepticism%20on%20Reddit%20|%20redditClimate_files/cluster_labels.png" width="40%">&lt;/img&gt;
</p>

<p>From this we can make a significant observation: that the 
r/climateskeptics fits into the political cluster more than into any of 
the more scientific or activist clusters.</p>

<h3 id="word-usage-trends">Word usage trends</h3>

<p>So far, our analysis of Reddit data has been <em>static</em> in time;
 we’ve been agregating data across time, but it’s also worth 
investigating how various metrics have changed over time. We chose to 
track word usage as measured by the frequency of posts containing that 
word.</p>

<p>For the subreddit of interest, we pull all the content using <code class="language-plaintext highlighter-rouge">pushshift_get</code> from <code class="language-plaintext highlighter-rouge">util.py</code>.
 This is then passed to a function that iterates through each post and 
checks if the post contains one of our words of interest:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_timeseries</span><span class="p">(</span><span class="n">content</span><span class="p">):</span>
    <span class="c1"># content is a list of submissions or comments
</span>    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">content</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
            <span class="c1"># check if the word is in the title, body or selftext
</span>            <span class="k">if</span> <span class="p">(</span><span class="s">'title'</span> <span class="ow">in</span> <span class="n">c</span> <span class="ow">and</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">c</span><span class="p">[</span><span class="s">'title'</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>\
            <span class="ow">or</span> <span class="p">(</span><span class="s">'selftext'</span> <span class="ow">in</span> <span class="n">c</span> <span class="ow">and</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">c</span><span class="p">[</span><span class="s">'selftext'</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>\
            <span class="ow">or</span> <span class="p">(</span><span class="s">'body'</span> <span class="ow">in</span> <span class="n">c</span> <span class="ow">and</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">c</span><span class="p">[</span><span class="s">'body'</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">()):</span>
                <span class="c1"># if so, add the created_utc field to the list
</span>                <span class="n">time_series</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="s">'created_utc'</span><span class="p">])</span>

        <span class="c1"># also maintain a reference of all content timestamps so that
</span>        <span class="c1"># we can compare the word frequency
</span>        <span class="n">reference_time_series</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="s">'created_utc'</span><span class="p">])</span>
</code></pre></div></div>

<p>This function populates <code class="language-plaintext highlighter-rouge">timeseries</code> which is a <code class="language-plaintext highlighter-rouge">dict</code> mapping each word to a list of UTC timestamps for the posts in that subreddit containing that word. We also populate <code class="language-plaintext highlighter-rouge">reference_time_series</code> which is a list of the timestamps of all posts.</p>

<p>If we wanted to calculate the fraction of posts which contain a 
certain word, say “apple”, we’d simply sum up the number of timestamps 
for that word and divide it by the number of reference timestamps.</p>

<p><code class="language-plaintext highlighter-rouge">apple_fraction = len(timeseries["apple"]) / len(reference_time_series)</code></p>

<p>However, since we’re interested in observing how this fraction may 
have changed over time, we need to sum up the timestamps within temporal
 bins. We can use <code class="language-plaintext highlighter-rouge">np.histogram</code>
 to count the number of timestamps within each bin and then normalize 
(divide each bin of the histogram by the reference bin) to find the 
fraction</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_histograms</span><span class="p">(</span><span class="n">num_bins</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">reference_time_series</span><span class="p">)</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">reference_time_series</span><span class="p">)</span>
    <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start_time</span><span class="p">,</span> <span class="n">end_time</span><span class="p">,</span> <span class="n">num_bins</span><span class="p">)</span>

    <span class="n">ref</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">reference_time_series</span><span class="p">,</span> <span class="n">bins</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="n">hist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">time_series</span><span class="p">[</span><span class="n">w</span><span class="p">],</span> <span class="n">bins</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">utc_to_year</span><span class="p">(</span><span class="n">bins</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">hist</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">/</span><span class="n">ref</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">w</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Post Date (yr)'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Percentage of posts containing word'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Word usage over time for r/{} {}s'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">subreddit</span><span class="p">,</span> <span class="n">endpoint</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>This function creates a normalized histogram for each word and plots the results, which are shown below for the <em>comment</em> and <em>submission</em> endpoints and two different subreddits</p>

<div align="center">
	<img alt="Submissions in the subreddit r/environmental_science that mention climate change or global warming" src="Climate%20Skepticism%20on%20Reddit%20|%20redditClimate_files/environmental_science_submissions_over_time.png" width="45%">
	<img alt="Comments in the subreddit r/environmental_science that mention climate change or global warming" src="Climate%20Skepticism%20on%20Reddit%20|%20redditClimate_files/environmental_science_comments_over_time.png" width="45%">
	<img alt="Submissions in the subreddit r/climateskeptics that mention climate change or global warming" src="Climate%20Skepticism%20on%20Reddit%20|%20redditClimate_files/climateskeptics_submissions_over_time.png" width="45%">
	<img alt="Comments in the subreddit r/climateskeptics that mention climate change or global warming" src="Climate%20Skepticism%20on%20Reddit%20|%20redditClimate_files/climateskeptics_comments_over_time.png" width="45%">
</div>

<p>We notice that accross endpoints, the trendlines look fairly similar,
 but across subreddits there is a noticeable difference. In 
r/environmental_science, the prevalence of “climate change” in posts has
 always dominated “global warming”. But in r/climateskeptics, “global 
warming” used to be a more common term, and over time was superceded by 
“climate change.” In <a href="https://trends.google.com/trends/explore?date=2010-11-10%202019-12-10&amp;geo=US&amp;q=climate%20change,global%20warming">google search trends</a>, we observe a similar inversion</p>

<p align="center">
	<img src="Climate%20Skepticism%20on%20Reddit%20|%20redditClimate_files/google_trends.png" width="80%">
</p>

<p>The terms “global warming” and “climate change” are often used 
interchangeably in the public lexicon, but in a scientific context they 
refer to two distint things. Perhaps that the discussion in 
r/enviromental_science shakes the trend is indicative that the 
discussion in that subreddit is more scientific in nature.</p>

<h3 id="linking">Linking</h3>

<p>What websites are commonly linked to in r/climateskeptics? How does 
this compare with r/sustainability? Which news sources are popular 
amongst r/climateskeptics submitters? In this section, we’ll uncover the
 data and try to answer these questions!</p>

<p>One particularly common type of reddit submission is one that is made
 up of a title and a link. Here’s an example of a r/climateskeptics 
submission that links to a Breitbart article:</p>

<p><img alt="r/climateskeptics submission with a link" src="Climate%20Skepticism%20on%20Reddit%20|%20redditClimate_files/reddit_link_example.png" width="500"></p>

<p>Programmatically, once we have a submission returned by a query to the pushshift API, we can access the full url by using the <code class="language-plaintext highlighter-rouge">"url"</code> field:
<code class="language-plaintext highlighter-rouge">"https://www.breitbart.com/europe/2019/12/08/happer-trump-understands-climate-change-is-mostly-hype/"</code></p>

<p>However, for our purposes, we only want the domain name. This is 
because using the domain name will allow us to count the number of times
 different domains are linked to in different subreddits. To get the 
domain name, we can use the value in the <code class="language-plaintext highlighter-rouge">"domain"</code> field. For the example submission above, this would be <code class="language-plaintext highlighter-rouge">"breitbart.com"</code>.</p>

<h4 id="popular-websites-by-subreddit">Popular Websites by Subreddit</h4>

<p>Here are the top 7 domains linked to from r/climateskeptics:</p>

<p><img alt="Most popular domains in r/climateskeptics submissions" src="Climate%20Skepticism%20on%20Reddit%20|%20redditClimate_files/domains_plot.png" width="500"></p>

<p>If you’re feeling adventurous, try going to some of those websites 
and delving into the world of climate skepticism (maybe not for too 
long).</p>

<h4 id="news">News</h4>

<p>The <a href="https://www.pewresearch.org/">Pew Research Center</a> conducted a <a href="https://www.journalism.org/2014/10/21/section-1-media-sources-distinct-favorites-emerge-on-the-left-and-right/">study</a>
 in which they rated the political stances of news outlets on the 
left-right political spectrum. Here’s their diagram illustrating their 
findings:</p>

<p><img alt="News outlets from left to right" src="Climate%20Skepticism%20on%20Reddit%20|%20redditClimate_files/pewstudy.png" width="500"></p>

<p>Let’s use this as a starting point, by defining a list with some of the more popular of these news domains from left to right:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">WORLD_DOMAINS_LEFT_TO_RIGHT</span> <span class="o">=</span> <span class="p">[</span><span class="s">"nytimes.com"</span><span class="p">,</span> <span class="c1"># the most left wing news outlet
</span>                               <span class="s">"bbc.com"</span><span class="p">,</span> <span class="s">"huffingtonpost.com"</span><span class="p">,</span> <span class="s">"washingtonpost.com"</span><span class="p">,</span>         
                               <span class="s">"cnn.com"</span><span class="p">,</span>
                               <span class="s">"nbcnews"</span><span class="p">,</span>
                               <span class="s">"news.google.com"</span><span class="p">,</span> <span class="s">"abcnews.go.com"</span><span class="p">,</span>
                               <span class="s">"wsj.com"</span><span class="p">,</span> <span class="c1"># a centrist news outlet
</span>                               <span class="s">"foxnews.com"</span><span class="p">,</span>
                               <span class="s">"breitbart.com"</span> <span class="c1"># the most right wing news outlet
</span>                               <span class="p">]</span>
</code></pre></div></div>

<p>We’ll also want to choose a list of subreddits to investigate. Let’s use a bunch of climate and environment related subreddits:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CLIMATE_SUBREDDITS</span> <span class="o">=</span> <span class="p">[</span><span class="s">"globalwarming"</span><span class="p">,</span> 
						  <span class="s">"globalclimatechange"</span><span class="p">,</span>
						  <span class="s">"environment"</span><span class="p">,</span>
						  <span class="s">"renewableenergy"</span><span class="p">,</span>
						  <span class="s">"climateskeptics"</span><span class="p">,</span>
						  <span class="s">"climatenews"</span><span class="p">,</span>
						  <span class="s">"climatechange"</span><span class="p">,</span>
						  <span class="s">"climateactionplan"</span>
						  <span class="p">]</span>
</code></pre></div></div>

<p>Great, now let’s write some code, starting with some imports:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">reddit</span> <span class="kn">import</span> <span class="n">query_n</span>
<span class="kn">from</span> <span class="nn">seaborn</span> <span class="kn">import</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<p>Now, let’s write code to count the number of submissions that link to each news outlet, by subreddit.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">news_domains_by_subreddit</span><span class="p">(</span><span class="n">category</span><span class="p">,</span> <span class="n">subreddits</span><span class="p">,</span> <span class="n">domains</span><span class="p">):</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s">"subreddit"</span><span class="p">:</span> <span class="s">","</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">subreddits</span><span class="p">),</span> <span class="s">"domain"</span><span class="p">:</span> <span class="s">","</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">domains</span><span class="p">)</span> <span class="p">}</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">query_n</span><span class="p">(</span><span class="n">category</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
    <span class="n">subreddit_counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">([</span> <span class="n">r</span><span class="p">[</span><span class="s">"subreddit"</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">])</span>

    <span class="n">news_subreddits</span> <span class="o">=</span> <span class="p">[</span><span class="n">subreddit</span> <span class="k">for</span> <span class="n">subreddit</span><span class="p">,</span><span class="n">_</span> <span class="ow">in</span> <span class="n">subreddit_counter</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s">"subreddit"</span><span class="p">:</span> <span class="s">","</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">news_subreddits</span><span class="p">),</span> <span class="s">"domain"</span><span class="p">:</span> <span class="s">","</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">domains</span><span class="p">)</span> <span class="p">}</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">query_n</span><span class="p">(</span><span class="n">category</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">20000</span><span class="p">)</span>
    <span class="n">subreddit_counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">([</span> <span class="n">r</span><span class="p">[</span><span class="s">"subreddit"</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">])</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">([(</span><span class="n">r</span><span class="p">[</span><span class="s">"domain"</span><span class="p">],</span> <span class="n">r</span><span class="p">[</span><span class="s">"subreddit"</span><span class="p">])</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="n">domain_count</span> <span class="o">=</span> <span class="n">counter</span><span class="p">[(</span><span class="n">r</span><span class="p">[</span><span class="s">"domain"</span><span class="p">],</span> <span class="n">r</span><span class="p">[</span><span class="s">"subreddit"</span><span class="p">])]</span>
        <span class="n">subreddit_total</span> <span class="o">=</span> <span class="n">subreddit_counter</span><span class="p">[</span><span class="n">r</span><span class="p">[</span><span class="s">"subreddit"</span><span class="p">]]</span>
        <span class="n">r</span><span class="p">[</span><span class="s">"proportion"</span><span class="p">]</span> <span class="o">=</span> <span class="n">domain_count</span><span class="o">/</span><span class="n">subreddit_total</span>

    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_records</span><span class="p">(</span><span class="n">results</span><span class="p">)[[</span><span class="s">"subreddit"</span><span class="p">,</span> <span class="s">"domain"</span><span class="p">,</span> <span class="s">"count"</span><span class="p">]]</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_news_domains_by_subreddit</span><span class="p">(</span><span class="n">subreddit_domains_df</span><span class="p">):</span>
    <span class="n">color_palette</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s">"coolwarm"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">domains</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">y</span> <span class="o">=</span> <span class="s">"proportion"</span><span class="p">,</span>
                     <span class="n">x</span> <span class="o">=</span> <span class="s">"subreddit"</span><span class="p">,</span> 
                     <span class="n">orient</span> <span class="o">=</span> <span class="s">"v"</span><span class="p">,</span>
                     <span class="n">hue</span> <span class="o">=</span> <span class="s">"domain"</span><span class="p">,</span> 
                     <span class="n">hue_order</span> <span class="o">=</span> <span class="n">domains</span><span class="p">,</span>
                     <span class="n">data</span> <span class="o">=</span> <span class="n">subreddit_domains_df</span><span class="p">,</span> 
                     <span class="n">palette</span> <span class="o">=</span> <span class="n">color_palette</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">f</span><span class="s">"# of news links"</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">f</span><span class="s">"subreddit"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">frameon</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s">'upper right'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">get_figure</span><span class="p">()</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">"subreddit_news_horizontal.png"</span><span class="p">)</span>
</code></pre></div></div>

<p>Finally, let’s run our code and plot the results:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">news_domains_by_subreddit</span><span class="p">(</span><span class="s">"submission"</span><span class="p">,</span> <span class="n">CLIMATE_SUBREDDITS</span><span class="p">,</span> <span class="n">LEFT_TO_RIGHT</span><span class="p">)</span>
<span class="n">plot_news_domains_by_subreddit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div>

<p>Here is the output plot. The news outlets colors range from dark blue
 to dark red where dark blue represents left wing politics and dark red 
represents right wing politics.</p>

<p><img alt="News outlets from left to right by subreddit" src="Climate%20Skepticism%20on%20Reddit%20|%20redditClimate_files/subreddit_news_horizontal.png" width="100%"></p>

<p>We can immediately see that r/climateskeptics submissions have a lot 
more right wing news outlet links. The tallest red bar represents 
breitbart.com, which is makes up over 40% of news links among the 
outlets we are considering.</p>

<p>Another interesting find is that while most other subreddits have 
good representation for a lot of different news outlets, 
r/GlobalClimateChange seems to have disproportionately many <washingtonpost.com> links to the Washington Post - over 60%!</washingtonpost.com></p>

<h3 id="sentiment-analysis">Sentiment Analysis</h3>

<p>Sentiment analysis is a technique that allows us to quantify the 
sentiment of different sentences. For example, we can use sentiment 
analysis to analyze how much positivity, negativity and neutrality a 
sentence has. Sentiment analysis models are usually supervised, meaning 
that they are learned or “trained” from labeled data. For example, we 
could train a sentiment analysis on the following labeled data:</p>

<table>
  <thead>
    <tr>
      <th>Sentence</th>
      <th>negativity</th>
      <th>neutrality</th>
      <th>positivity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>We cleaned 40 pounds of trash from our beach ! (<a href="https://www.reddit.com/r/DeTrashed/comments/e6ys1g/we_cleaned_40_pounds_of_trash_from_our_beach/">link</a>)</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <td>Getting reaaally tired of these alarmist headlines all over the media in my country. (<a href="https://www.reddit.com/r/climateskeptics/comments/e96wig/getting_reaaally_tired_of_these_alarmist/">link</a>)</td>
      <td>0.182</td>
      <td>0.818</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>

<p>In practice, we would need a lot more labeled examples like these in 
order to train a performant sentiment analyzer. Unfortuntatly, we don’t 
have any such labeled examples (we don’t have any 
positivity/negativity/neutrality scores)!</p>

<p>Luckily, others have compiled large datasets of sentences and 
manually labeled them with sentiment scores. They then trained the 
sentiment analysis model on that data and made the trained model 
available to the public (that’s us)!</p>

<p>The specific model we’re going to use is called <a href="https://www.nltk.org/api/nltk.sentiment.html#module-nltk.sentiment.vader">VADER</a>, and is part of python’s Natural Language Toolkit (nltk).</p>

<p>Let’s start by importing the the sentiment analyzer and a few other 
libraries. In particular we’ll also want to import nltk’s sentence 
tokenizer, which splits text into sentences and splits each sentence 
into a list of tokens, which can be words or punctuation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">reddit</span> <span class="kn">import</span> <span class="n">query_n</span><span class="p">,</span> <span class="n">BIG_LIST</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="kn">from</span> <span class="nn">nltk.sentiment.vader</span> <span class="kn">import</span> <span class="n">SentimentIntensityAnalyzer</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
</code></pre></div></div>

<p>Next, let’s define a function that queries submissions and comments, 
performs sentiment analysis, and then computes the averages 
positivity/negativity/neutrality scores accross all of the sentences of 
each subreddit:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sentiment_analysis</span><span class="p">(</span><span class="n">category</span><span class="p">,</span> <span class="n">subreddit</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">"inf"</span><span class="p">)):</span>
    <span class="n">all_sentences</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">query_n</span><span class="p">(</span><span class="n">category</span><span class="p">,</span> <span class="p">{</span><span class="s">"subreddit"</span><span class="p">:</span> <span class="n">subreddit</span><span class="p">},</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="n">text_fields</span> <span class="o">=</span> <span class="p">[</span><span class="s">"title"</span><span class="p">,</span> <span class="s">"selftext"</span><span class="p">,</span> <span class="s">"body"</span><span class="p">]</span>
        <span class="n">submissions</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="p">[</span><span class="n">field</span><span class="p">]</span> <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">text_fields</span> <span class="k">if</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">result</span><span class="p">]</span>
        <span class="n">submission_text</span> <span class="o">=</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
        <span class="n">sentences</span> <span class="o">=</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">submission_text</span><span class="p">)</span>
        <span class="n">all_sentences</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>

    <span class="n">sentiment_analyzer</span> <span class="o">=</span> <span class="n">SentimentIntensityAnalyzer</span><span class="p">()</span>
    <span class="n">score_sums</span> <span class="o">=</span> <span class="p">{</span><span class="s">"pos"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">"neu"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">"neg"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">"compound"</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">all_sentences</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">"Skipping {subreddit} (no submissions)"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">score_sums</span>

    <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">all_sentences</span><span class="p">:</span>
        <span class="n">polarity</span> <span class="o">=</span> <span class="n">sentiment_analyzer</span><span class="o">.</span><span class="n">polarity_scores</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">score_category</span> <span class="ow">in</span> <span class="n">polarity</span><span class="p">:</span>
            <span class="n">score_sums</span><span class="p">[</span><span class="n">score_category</span><span class="p">]</span> <span class="o">+=</span> <span class="n">polarity</span><span class="p">[</span><span class="n">score_category</span><span class="p">]</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Subreddit:"</span><span class="p">,</span> <span class="n">subreddit</span><span class="p">)</span>
    <span class="n">score_avgs</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="nb">round</span><span class="p">(</span><span class="n">v</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">all_sentences</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">score_sums</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">"Num sentences: {len(all_sentences)}"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">score_avgs</span>
</code></pre></div></div>

<p>Now, let’s call our <code class="language-plaintext highlighter-rouge">sentiment_analysis</code> function on each of the climate related subreddits from our curated list:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">exec_sentiment_analysis</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">"inf"</span><span class="p">)):</span>
    <span class="n">records</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">subreddit</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">BIG_LIST</span><span class="p">):</span>
        <span class="n">score_avgs</span> <span class="o">=</span> <span class="n">sentiment_analysis</span><span class="p">(</span><span class="s">"comment"</span><span class="p">,</span> <span class="n">subreddit</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">n</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">score_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s">"compound"</span><span class="p">,</span> <span class="s">"neu"</span><span class="p">,</span> <span class="s">"pos"</span><span class="p">,</span> <span class="s">"neg"</span><span class="p">]:</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">score_avgs</span><span class="p">[</span><span class="n">score_type</span><span class="p">]</span>
            <span class="n">record</span> <span class="o">=</span> <span class="p">{</span><span class="s">"subreddit"</span><span class="p">:</span> <span class="n">subreddit</span><span class="p">,</span> <span class="s">"score_type"</span><span class="p">:</span> <span class="n">score_type</span><span class="p">,</span> <span class="s">"score"</span><span class="p">:</span> <span class="n">score</span><span class="p">}</span>
            <span class="n">records</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>

    <span class="n">sentiment_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_records</span><span class="p">(</span><span class="n">records</span><span class="p">)</span>
    <span class="n">sentiment_df</span><span class="o">.</span><span class="n">to_pickle</span><span class="p">(</span><span class="s">"sentiment_df.pickle"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">sentiment_df</span>
</code></pre></div></div>

<p>Finally, let’s write some code to plot the subreddits in order of 
their sentiment scores (e.g. most negative subreddits, most positive 
subreddits):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_sentiment_analysis</span><span class="p">(</span><span class="n">sentiment_df</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">sort_score_type</span> <span class="o">=</span> <span class="s">"pos"</span><span class="p">):</span>
    <span class="n">sentiment_df</span> <span class="o">=</span> <span class="n">sentiment_df</span> <span class="ow">or</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s">"sentiment_df.pickle"</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
    <span class="n">f</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span>
    		<span class="mi">15</span><span class="p">,</span>
    		<span class="mi">15</span><span class="p">,</span>
    		<span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">60</span><span class="p">,</span> <span class="mi">60</span><span class="p">),</span>
    		<span class="n">sharex</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
    		<span class="n">sharey</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
    		<span class="n">squeeze</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
    
    <span class="n">ax_array</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">sorter</span><span class="p">(</span><span class="n">subreddit</span><span class="p">):</span>
    	<span class="n">df</span> <span class="o">=</span> <span class="n">sentiment_df</span>
    	<span class="n">select_subreddit</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">"subreddit"</span><span class="p">]</span> <span class="o">==</span> <span class="n">subreddit</span>
    	<span class="n">select_score_type</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">"score_type"</span><span class="p">]</span> <span class="o">==</span> <span class="n">sort_score_type</span>
    	<span class="k">return</span> <span class="n">df</span><span class="p">[</span><span class="n">select_subreddit</span> <span class="o">&amp;</span> <span class="n">select_score_type</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s">"score"</span><span class="p">]</span>
    	
    <span class="n">sorted_subreddits</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">BIG_LIST</span><span class="p">,</span> <span class="n">reverse</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="n">sorter</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">subreddit</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sorted_subreddits</span><span class="p">):</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span>
        	<span class="n">x</span><span class="o">=</span><span class="s">"subreddit"</span><span class="p">,</span>
        	<span class="n">y</span><span class="o">=</span><span class="s">"score"</span><span class="p">,</span>
        	<span class="n">hue</span><span class="o">=</span><span class="s">"score_type"</span><span class="p">,</span>
        	<span class="n">hue_order</span><span class="o">=</span><span class="p">[</span><span class="s">"neg"</span><span class="p">,</span> <span class="s">"neu"</span><span class="p">,</span> <span class="s">"pos"</span><span class="p">],</span>
        	<span class="n">data</span><span class="o">=</span><span class="n">sentiment_df</span><span class="p">[</span><span class="n">sentiment_df</span><span class="p">[</span><span class="s">"subreddit"</span><span class="p">]</span> <span class="o">==</span> <span class="n">subreddit</span><span class="p">],</span>
        	<span class="n">ax</span> <span class="o">=</span> <span class="n">ax_array</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="n">subreddit</span><span class="p">)</span>
        
    <span class="n">f</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">f</span><span class="s">"sentiment_plot_{sort_key}.png"</span><span class="p">)</span>
</code></pre></div></div>

<p>The Top 5 Most <strong>POSITIVE</strong> Subreddits:
<img alt="r/climateskeptics submission with a link" src="Climate%20Skepticism%20on%20Reddit%20|%20redditClimate_files/sentiment_top_pos.png" width="100%"></p>

<p>The Top 5 Most <strong>NEGATIVE</strong> Subreddits:
<img alt="r/climateskeptics submission with a link" src="Climate%20Skepticism%20on%20Reddit%20|%20redditClimate_files/sentiment_top_neg.png" width="100%"></p>

<h2 id="related-work">Related Work</h2>

<p>Before settling on Reddit, we considered a number of other social 
media platforms and sources of data. We decided to pursue Reddit, 
because we found that a number of studies have already attempted to 
characterize climate skepticism on those platforms. We’ll mention a 
number of those papers and articles here, as they informed our 
experiments and provide interesting perspectives</p>

<ul>
  <li><a href="https://www.mdpi.com/2071-1050/9/11/2019/pdf">S. Stepchenkova et al.</a> analyze the quality of crowdsourced data vs data collected by paid workers about climate change discourse on Twitter.</li>
  <li><a href="https://medium.com/neo4j/climate-change-twitter-analysis-2016-cbc6c1fd8f1a">John Swain’s Medium Article</a> provides some beautiful visualizations and an in depth discussion of the flow of climate conversation through the Twitter graph</li>
  <li><a href="https://www.unglobalpulse.org/projects/Twitter-Climate-Change">This UN Global Pulse study</a>
 found that the Global Climate March and Global Climate Summit in 2014 
significantly increased the amount of conversation about climate change 
on twitter</li>
  <li><a href="https://www.tandfonline.com/doi/full/10.1080/17524032.2018.1527378?needAccess=true">E. Bloomfield et al.</a> studies climate skeptic communities on Facebook. They find that posters provide links to give the appearance of credibility</li>
  <li><a href="https://thebulletin.org/2019/08/why-facebook-youtube-and-twitter-are-bad-for-the-climate/">Dawn Stover’s article</a> explains how Facebook and Twitter facilitate the spread of climate missinformation</li>
  <li><a href="https://envirodatagov.org/new-digital-landscape/">This Environmental Data &amp; Governance Initiative report</a> reveals that the term “climate change” has been dissapearing from government websites under the Trump Administration</li>
</ul>


      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        
        <p class="copyright">redditClimate maintained by <a href="https://github.com/RedditClimate">RedditClimate</a></p>
        
        <p>Published with <a href="https://pages.github.com/">GitHub Pages</a></p>
      </footer>
    </div>

    
  

</body></html>